{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Intro_Opt_ML_STUDENTS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sysbio-unam/GAFerm/blob/main/Copy_of_Intro_Opt_ML_STUDENTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlT5RZ2KxQ5h"
      },
      "source": [
        "# Authors:\n",
        "## Antonio del Rio Chanona      https://www.imperial.ac.uk/people/a.del-rio-chanona\n",
        "## Edgar Ivan Sanchez Medina    https://www.mpi-magdeburg.mpg.de/person/103552/2316\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "from matplotlib.animation import FuncAnimation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTgaIpQpzJXO"
      },
      "source": [
        "Along this notebook you will find <font color='blue'>text in blue that describe the **coding tasks** that you have to implement.</font> Within the code cell, your implementation needs to be between the line:\n",
        "\n",
        "`#-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-#-#`\n",
        "\n",
        "and the line:\n",
        "\n",
        "`#-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#-#-#`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HznFdIzf0rKR"
      },
      "source": [
        "# **1. Optimization basics**\n",
        "\n",
        "An optimization problem has the form\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\min_{x \\in X} \\quad & f(x)\\\\\n",
        "\\textrm{s.t.} \\quad & g_i(x) \\leq 0, i= 1, ..., m\\\\\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "The vector $x = (x_1,..., x_n)$ is the optimization variable of the problem, the function $f : \\mathbb{R}^n \\to \\mathbb{R} $ is the objective function, the functions $g_i : \\mathbb{R}^n \\to \\mathbb{R} $,\n",
        "$i = 1, ...,m$, are the (inequality) constraint functions. A vector $x^â‹†$ is called optimal, if it has the smallest objective value among all vectors\n",
        "that satisfy the constraints.\n",
        "\n",
        "**Convex optimization**\n",
        "\n",
        "A convex optimization problem is one in which the objective and the\n",
        "constraint functions are convex, which means they satisfy the inequality:\n",
        "\n",
        "\\begin{equation}\n",
        "f(\\alpha x + \\beta y) \\leq \\alpha f(x) + \\beta f(y)\n",
        "\\end{equation}\n",
        "\n",
        "for all $x,y \\in \\mathbb{R}^n$ and all $\\alpha, \\beta \\in \\mathbb{R}$ with $\\alpha + \\beta = 1$, $\\alpha  \\geq 0$ and $\\beta  \\geq 0$\n",
        "\n",
        "It is worth nothing that any equality constraint can be represented as two inequality constraints. This enforce that, in a convex problem, the equality constraints must be linear.\n",
        "\n",
        "Therefore, an optimization problem is convex whenever the following requirements are met:\n",
        "\n",
        "* The objective function is convex.\n",
        "* The inequality constraints are convex.\n",
        "* The equality constraints are linear.\n",
        "\n",
        "**Global and local optima**\n",
        "\n",
        "An optimal solution $x^*$ is said to be the global optimum when the constraints are met at this point and $f(x^*) \\leq f(x),  \\forall x \\in X$. When this condition is met only whitin a certain neightborhood, the solution is called local optimum.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkbFE6Kd091N"
      },
      "source": [
        "# **2. Gradient descent**\n",
        "\n",
        "The optimization methods known as \"descent methods\" minimize a function by applying the following update rule at each iteration:   \n",
        "\n",
        "\\begin{equation}\n",
        "x^{(k+1)} = x^{(k)} +  \\alpha^{(k)} \\Delta x^{(k)}\n",
        "\\end{equation}\n",
        "\n",
        "in this rule, $\\Delta^{(k)}$ denotes the **direction** at the iteration $k$, and $\\alpha^{(k)} \\geq 0$ is a scalar value called **step size**. These methods are called descent methods, because at each iteration $f \\left(x^{(k+1)} \\right) \\leq  f \\left(x^{(k)} \\right)$.\n",
        "\n",
        "In the method called *Gradient Descent* the direction is chosen to be the negative of the gradient: $ \\Delta x := - \\nabla f(x) $. therefore, the algorithm is:\n",
        "\n",
        "**Algorithm**\n",
        "\n",
        "1. Given a starting point $x \\in X$.\n",
        "2. Repeat\n",
        "3. $~~~~~~$ $ \\Delta x := - \\nabla f(x) $\n",
        "2. $~~~~~~$ Choose $\\alpha$.\n",
        "3. $~~~~~~$ Update: $ x = x +  \\alpha \\Delta x$\n",
        "4. until stopping criterion is met.\n",
        "\n",
        "<font color='blue'>Code step 3 and 5 within the following code.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enTMXZt2xi-V"
      },
      "source": [
        "############################\n",
        "# --- Gradient Descent --- #\n",
        "############################\n",
        "\n",
        "def gradient_descent(f, x0, grad_f, lr, max_iter=1e5, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    Gradient Descent\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        lr       : Learning rate\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        plot     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    \n",
        "    # plotting\n",
        "    if traj == True:\n",
        "        x_list = []\n",
        "        f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter:    \n",
        "        \n",
        "        #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-#-#\n",
        "        grad_i  = 00000     # compute gradient\n",
        "        x       = 00000     # compute step    \n",
        "        #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#-#\n",
        "\n",
        "        iter_i += 1\n",
        "        \n",
        "        # plotting\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "        \n",
        "    print(' Optimization using Gradient Descent \\n')\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "    \n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list, \n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOW1KIy7R2Wr"
      },
      "source": [
        "One way to approximate the (numerically) gradient of a function  is the **finite diffirences method**. There exist mainly three type of finite difference approximations:\n",
        "\n",
        "* Backward difference  $f'(x) \\approx \\frac{f(x_k) - f(x_k - \\epsilon)}{\\epsilon}$\n",
        "* Forward difference  $f'(x) \\approx \\frac{f(x_k + \\epsilon) + f(x_k)}{\\epsilon}$\n",
        "* Central difference $f'(x) \\approx \\frac{f(x_k + \\frac{\\epsilon}{2}) - f(x_k - \\frac{\\epsilon}{2})}{\\epsilon}$\n",
        "\n",
        "However, the central difference approximation gives the most accurate one among these three. Therefore, let's implement that one here.\n",
        "\n",
        "<font color='blue'>Implement the **central finite differences**.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMycAmM0SOji"
      },
      "source": [
        "######################################    \n",
        "# --- Central finite differences --- #\n",
        "######################################\n",
        "\n",
        "def central_finite_diff(f, x):\n",
        "      '''\n",
        "      Central finite differences approximation.\n",
        "      INPUTS:\n",
        "          f  : Function\n",
        "          x  : Position where to approximate the gradient\n",
        "      OUTPUTS:\n",
        "          grad: Approximation of the gradient of f at x \n",
        "      '''\n",
        "      dim = x.shape[0]\n",
        "      eps  = np.sqrt(np.finfo(float).eps)  # Step-size is the square root of the machine precision\n",
        "      grad = np.zeros((1,dim))\n",
        "      \n",
        "      #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-#-#\n",
        "      for i in range(dim):\n",
        "          e           = np.zeros((1,dim))\n",
        "          e[0,i]        = eps\n",
        "          grad_approx = (f(x + e/2) - f(x - e/2))/eps\n",
        "          grad[0,i]     = grad_approx\n",
        "      \n",
        "      #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#-#-#\n",
        "      return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6p9RFf9EsWj"
      },
      "source": [
        "The next cell contains the test function that we are going to use here, but this can be replace by any function. In order to approximate the gradients we are going to use the central finite differences method with five-points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTes4LZfzw96"
      },
      "source": [
        "def Rosenbrock_f(x):\n",
        "    '''\n",
        "    Rosenbrock function\n",
        "    '''\n",
        "    n = np.shape(x)[1]\n",
        "    z = np.sum(100*(x[:,1:] - x[:,:n-1]**2)**2 + (x[:,:n-1] - 1)**2, axis=1)\n",
        "    return z\n",
        "\n",
        "###############################################\n",
        "# --- Central finite differences 5 points --- # \n",
        "###############################################\n",
        "\n",
        "def central_finite_diff5(f, x):\n",
        "      '''\n",
        "      Five-points method for central finite differences.\n",
        "      INPUTS:\n",
        "          f  : Function\n",
        "          x  : Position where to approximate the gradient\n",
        "      OUTPUTS:\n",
        "          grad: Approximation of the gradient of f at x \n",
        "      '''\n",
        "      dim = x.shape[1]\n",
        "      # Step-size is taken as the square root of the machine precision\n",
        "      eps  = np.sqrt(np.finfo(float).eps) \n",
        "      grad = np.zeros((1,dim))\n",
        "        \n",
        "      for i in range(dim):\n",
        "          e           = np.zeros((1,dim))\n",
        "          e[0,i]      = eps\n",
        "          grad_approx = (f(x - 2*e) - 8*f(x - e) + 8*f(x + e) - f(x + 2*e) )/(12*eps) \n",
        "          \n",
        "          grad[0,i]     = grad_approx\n",
        "        \n",
        "      return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dlj9-ndyPhU"
      },
      "source": [
        "# --- Gradient Descent --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = gradient_descent(Rosenbrock_f, x0, central_finite_diff5, 0.001, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-sSAU1qEBqn"
      },
      "source": [
        "# Plot function\n",
        "x_1 = np.linspace(0,1)\n",
        "x_2 = np.linspace(0,1)\n",
        "X, Y = np.meshgrid(x_1, x_2)\n",
        "Z = Rosenbrock_f(np.append(X.reshape(-1,1), Y.reshape(-1,1), axis=1))\n",
        "Z = Z.reshape(X.shape)\n",
        "\n",
        "x_list    = np.array(x_list).reshape(-1,x0.shape[1])\n",
        "\n",
        "x_summary = []\n",
        "f_summary = []\n",
        "for i in range(x_list.shape[0]):\n",
        "  if i % 100 == 0:\n",
        "    x_summary.append(x_list[i])\n",
        "    f_summary.append(f_list[i])\n",
        "x_summary = np.array(x_summary).reshape(-1,x0.shape[1])\n",
        "\n",
        "fig = plt.figure(figsize=(6,5))\n",
        "left, bottom, width, height = 0.15, 0.1, 0.8, 0.8\n",
        "ax = fig.add_axes([left, bottom, width, height]) \n",
        "contours = ax.contour(x_1, x_2, Z, colors='black', alpha=0.8)\n",
        "ax.clabel(contours, inline=True, fontsize=8)\n",
        "ax.set_xlabel(r'$x_1$')\n",
        "ax.set_ylabel(r'$x_2$')\n",
        "display_value = ax.text(0.05, 0.2, '', transform=ax.transAxes)\n",
        "plt.close()\n",
        "\n",
        "def animate(i):\n",
        "    ax.plot(x_summary[:i, 0], x_summary[:i, 1], 'k.', alpha=0.6)    # Animate points\n",
        "    display_value.set_text('Min = ' + str(f_summary[i]))          # Animate display value\n",
        "    ax.set_title('Rosenbrock function, Iteration: ' + str(i*100))  # Animate title\n",
        "    return display_value\n",
        "\n",
        "anim = FuncAnimation(fig, animate, frames=len(f_summary), interval=100, repeat_delay=800)\n",
        "\n",
        "HTML(anim.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOrePufC1LU3"
      },
      "source": [
        "# **4. Gradient descent with momentum**\n",
        "\n",
        "The idea behind this extension is to avoid most of the (unnecesary) zig-zag movements of Gradient descent by accumulating momentum along the direction towards the optimum while we iterate. \n",
        "\n",
        "Therefore, the update rule of Gradient Descent is modified like this:\n",
        "\n",
        "\\begin{equation}\n",
        "x^{(k+1)} = x^{(k)} + v^{(k)}\n",
        "\\end{equation}\n",
        "\n",
        "where $v^{(k)}$ is the velocity term defined by:\n",
        "\n",
        "\\begin{equation}\n",
        "v^{(k)} = \\beta v^{(k-1)} - \\alpha \\nabla f(x^{(k)}) \n",
        "\\end{equation}\n",
        "\n",
        "where $\\beta \\in [0,1]$ is the momentum hyperparameter commonly set to 0.9."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxCQhvBuycUd"
      },
      "source": [
        "<font color='blue'>Code a function for the **velocity** term calculation and a function for the **line search** method. </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2G0TK28byqyb"
      },
      "source": [
        "####################\n",
        "# --- Momentum --- #\n",
        "####################\n",
        "\n",
        "def momentum(grad_i, v_prev, lr, beta=0.9):\n",
        "    '''\n",
        "    Momentum function\n",
        "    INPUTS:\n",
        "        grad_i  : Gradient of function at current position\n",
        "        v_prev  : velocity value at the previous position\n",
        "        beta    : Momentum hyperparameter\n",
        "    OUTPUTS:\n",
        "        v       : Velocity term\n",
        "    '''\n",
        "    #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-#-#\n",
        "    v = 00000 # velocity term\n",
        "    #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#-#\n",
        "\n",
        "    return v\n",
        "\n",
        "#######################\n",
        "# --- Line search --- #\n",
        "#######################\n",
        "\n",
        "def ls(grad_i, x, f):\n",
        "    '''\n",
        "    Line search for determining learning rate\n",
        "    INPUTS:\n",
        "        grad_i  : Gradient of function at current position\n",
        "        x       : Current position\n",
        "        f       : Objective function\n",
        "    OUTPUTS:\n",
        "        lr    : Optimal learning rate\n",
        "        iter  : Number of iterations needed in line search\n",
        "    '''\n",
        "    iter = 0\n",
        "    lr   = 1\n",
        "    #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-#-#\n",
        "    # compute line search loop\n",
        "    while f(x) > f(x - lr*grad_i) and iter<100:\n",
        "        lr  = 0.5*lr          \n",
        "    #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#-#\n",
        "        iter += 1\n",
        "    \n",
        "    return lr, iter\n",
        "\n",
        "#######################\n",
        "# --- Line search --- #\n",
        "#######################\n",
        "\n",
        "def line_search(grad_i, x, f, A=0.1, B=0.8):\n",
        "    '''\n",
        "    Line search for determining learning rate\n",
        "    INPUTS:\n",
        "        grad_i  : Gradient of function at current position\n",
        "        x       : Current position\n",
        "        f       : Objective function\n",
        "    OUTPUTS:\n",
        "        lr    : Optimal learning rate\n",
        "        iter  : Number of iterations needed in line search\n",
        "    '''\n",
        "    #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-#-#\n",
        "    iter = 0\n",
        "    lr   = 1\n",
        "    while f(x - lr*grad_i) > f(x) - A*lr*np.dot(grad_i,grad_i.T) and iter<100:\n",
        "        lr  = B*lr\n",
        "        iter += 1\n",
        "    #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#-#\n",
        "    \n",
        "    return lr, iter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMfT7e6lyxqR"
      },
      "source": [
        "##########################################################\n",
        "# --- Gradient Descent with line search and momentum --- #\n",
        "##########################################################\n",
        "\n",
        "def GD_ls_momentum(f, x0, grad_f, beta=0.9, max_iter=1e5, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    Gradient Descent with line search and momentum\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        beta     : Parameter beta for the momentum calculation\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        plot     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    \n",
        "    # plotting\n",
        "    if traj == True:\n",
        "        x_list = []\n",
        "        f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    v_prev = 0      # initialize at zero to get normal GD at first step\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter:    \n",
        "        grad_i  = grad_f(f,x)                               # compute gradient\n",
        "        lr      = line_search(grad_i, x, f)[0]              # compute learning rate using line search\n",
        "        v       = momentum(grad_i, v_prev, lr, beta=beta)   # compute momentum\n",
        "        x       = x + v                                     # compute step \n",
        "        v_prev  = v                                         # update previous momentum term   \n",
        "        iter_i += 1\n",
        "        \n",
        "        # plotting\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "        \n",
        "    print(' Optimization using Gradient Descent with momentum \\n')\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "    \n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list, \n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScRGyx0tzx-j"
      },
      "source": [
        "# --- Gradient Descent with line search and momentum --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = GD_ls_momentum(Rosenbrock_f, x0, central_finite_diff5, beta=0.95, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYNQeRLN0ZUO"
      },
      "source": [
        "x_array = np.array(x_list).reshape(-1,2)\n",
        "fig  = plt.figure()\n",
        "cs   = plt.scatter(x_array[:,0], x_array[:,1], marker=\".\", c=f_list , cmap=\"seismic\")\n",
        "cbar = fig.colorbar(cs)\n",
        "plt.xlabel(r'$x_1$'); plt.ylabel(r'$x_2$'); plt.title('Rosenbrock with line search and momentum')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuCBZ6Hb12l5"
      },
      "source": [
        "# **5. Nesterov Accelerated Gradient Descent**\n",
        "\n",
        "The Nesterov Accelerated Gradient Descent (NAG) is a further improvement to the Gradient Descent with momentum algorithm. The step direction in NAG is calculated based on the gradient on an approximated future position instead of the current position, in this way, more gradient information is included into the update step compared to the traditional momentum approach.\n",
        "\n",
        "Therefore, the velocity term in NAG is determined by:\n",
        "\n",
        "$v^{(k)} = \\beta v^{(k-1)} - \\alpha \\nabla f(\\tilde{x}^{(k)})$\n",
        "\n",
        "where $\\tilde{x}^{(k)}$ is the approximated future position calculated as:\n",
        "\n",
        "$\\tilde{x}^{(k)} = x^{(k)} + \\beta v^{(k-1)}$\n",
        "\n",
        "<font color='blue'>Code a function for the **nesterov** calculation of velocity. </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbrQjOmLGfi1"
      },
      "source": [
        "####################\n",
        "# --- Nesterov --- #\n",
        "####################\n",
        "\n",
        "def nesterov(grad_tilde, v_prev, lr, beta=0.9):\n",
        "    '''\n",
        "    Momentum function\n",
        "    INPUTS:\n",
        "        grad_tilde  : Gradient of function at nesterov modified position\n",
        "        v_prev      : velocity value at the previous position\n",
        "        beta        : Momentum hyperparameter\n",
        "    OUTPUTS:\n",
        "        v           : Velocity term\n",
        "    '''\n",
        "    #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-# \n",
        "    v = 00000  # velocity term\n",
        "    #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#-#\n",
        "\n",
        "    return v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTHkgNmOGrpP"
      },
      "source": [
        "<font color='blue'>Implement your function **`nesterov` into the `NAG`** function bellow</font>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC6n6HwHGjHZ"
      },
      "source": [
        "#################################\n",
        "# --- NAG with line search  --- #\n",
        "#################################\n",
        "\n",
        "def NAG(f, x0, grad_f, beta=0.9, max_iter=1e5, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    Nesterov Accelerated Gradient Descent with line search\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        beta     : Parameter beta for the momentum calculation\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        plot     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    \n",
        "    # plotting\n",
        "    if traj == True:\n",
        "        x_list = []\n",
        "        f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    v_prev = 0      # initialize at zero to get normal GD-momentum at first step\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter:    \n",
        "        grad_i  = grad_f(f,x)                               # compute gradient at current position\n",
        "        \n",
        "        #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-# \n",
        "        x_tilde = 00000                            # nesterov modified position\n",
        "        g_tilde = 00000                          # compute gradient of function at x_tilde\n",
        "        lr      = line_search(g_tilde, x, f)[0]              # compute learning rate using line search\n",
        "        v       = 00000   # compute momentum\n",
        "        #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-# \n",
        "\n",
        "        x       = x + v                                     # compute step \n",
        "        v_prev  = v                                         # update previous momentum term   \n",
        "        iter_i += 1\n",
        "        \n",
        "        # plotting\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "        \n",
        "    print(' Optimization using NAG \\n')\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "    \n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list, \n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxD3RqIgG5Go"
      },
      "source": [
        "# --- NAG --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = NAG(Rosenbrock_f, x0, central_finite_diff5, beta=0.95, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2hRk4oFHFDx"
      },
      "source": [
        "x_array = np.array(x_list).reshape(-1,2)\n",
        "fig  = plt.figure()\n",
        "cs   = plt.scatter(x_array[:,0], x_array[:,1], marker=\".\", c=f_list , cmap=\"seismic\")\n",
        "cbar = fig.colorbar(cs)\n",
        "plt.xlabel(r'$x_1$'); plt.ylabel(r'$x_2$'); plt.title('Rosenbrock with NAG')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48plrpuJ2Wts"
      },
      "source": [
        "# **8. RMSProp**\n",
        "\n",
        "RMSProp was proposed by Geoffrey Hinton in his [Coursera lecture](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf) and it is pretty much the same as the idea 1 presented at the [AdaDelta paper](https://arxiv.org/pdf/1212.5701.pdf). This idea is: adapt the learning rate using the gradiet information of the previous $w$ steps by computing the average exponential decay. Hence, RMSprop update rule is:\n",
        "\n",
        "$x^{(k)} = x^{(k)} - \\alpha \\frac{\\nabla f(x^{(k)})}{V^{(k)}}$\n",
        "\n",
        "where \n",
        "\n",
        "$V^{(k)} = \\sqrt{\\rho V^{(k-1)} + (1-\\rho) \\left(\\nabla f(x^{(k)})\\right)^2 + \\epsilon}$\n",
        "\n",
        "For the sake of completeness let's also implement it in such form.\n",
        "\n",
        "<font color='blue'>Code the **accumulation of squared gradients** of RMSProp.</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0OycGIaMggi"
      },
      "source": [
        "####################\n",
        "# --- RMSProp --- #\n",
        "####################\n",
        "\n",
        "def rmsprop(f, x0, grad_f, lr=0.001, rho=0.95, eps=1e-8, max_iter=1e5, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    RMSProp optimization algorithm\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        rho      : Exponential decay parameter\n",
        "        eps      : Small constant to avoid division over zero\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        plot     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    \n",
        "    # plotting\n",
        "    if traj == True:\n",
        "        x_list = []\n",
        "        f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    V = 0\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter:    \n",
        "        grad_i  = grad_f(f,x)                                           # compute gradient\n",
        "        #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#\n",
        "        V       = 00000          # exponential decay average on gradients\n",
        "        #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#\n",
        "        x       = x - lr*grad_i/V                                       # compute step\n",
        "        iter_i += 1\n",
        "        \n",
        "        # plotting\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "        \n",
        "    print(' Optimization using RMSProp \\n')\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "    \n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list, \n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dex0AysDMmTT"
      },
      "source": [
        "# --- RMSProp --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = rmsprop(Rosenbrock_f, x0, central_finite_diff5, rho=0.9, eps=1e-8, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCKz5yfQNI8_"
      },
      "source": [
        "x_array = np.array(x_list).reshape(-1,2)\n",
        "fig  = plt.figure()\n",
        "cs   = plt.scatter(x_array[:,0], x_array[:,1], marker=\".\", c=f_list , cmap=\"seismic\")\n",
        "cbar = fig.colorbar(cs)\n",
        "plt.xlabel(r'$x_1$'); plt.ylabel(r'$x_2$'); plt.title('Rosenbrock with RMSProp')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQbnc_yQNEBa"
      },
      "source": [
        "Well, this is a surprise, the algorithm converged to the optimum if we use the RMSProp form! \n",
        "\n",
        "Similar result was encounter in the [book](https://books.google.de/books?id=IbnEDwAAQBAJ&pg=PA189&lpg=PA189&dq=adadelta+not+converging&source=bl&ots=f2i8liEovl&sig=ACfU3U2nzVAPCLLtC3Os_cxmHmh7acOBww&hl=en&sa=X&ved=2ahUKEwjl4YLJ7MTqAhWNs4sKHQUfCrQQ6AEwA3oECAgQAQ#v=onepage&q=rmsprop&f=false) we mentioned aboved. The reason is that the accumulation of the parameters update ($E^{(k-1)}$ in AdaDelta) can act as an accelerator term at the fist iterations. However, when approximating the optimum, this same \"kind of momentum\" prevents the algorithm from convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7Nt4ZaR2bpx"
      },
      "source": [
        "# **9. Adam**\n",
        "\n",
        "Adam combines the nice key property of the momentum methods with the adaptive learning rate methods. In addition to keep the accumulation term of the squared gradients $V^{(k)}$, Adam also has an accumulation term for past gradients (like the momentum methods).\n",
        "\n",
        "The strategy of Adam is to calculate two moments for the gradients:\n",
        "\n",
        "*   First moment (mean): \n",
        "\n",
        "$~~~~~~~~~~~~~~~~~ m^{(k)}=\\beta_1 m^{(k-1)} + (1-\\beta_1) \\nabla f(x^{(k)})$\n",
        "\n",
        "*   Second moment (uncentered variance):\n",
        "\n",
        "$~~~~~~~~~~~~~~~~~ V^{(k)}= \\beta_2 V^{(k-1)} + (1-\\beta_2) \\left( \\nabla f(x^{(k)}) \\right)^2$\n",
        "\n",
        "where $\\beta_1$ and $\\beta_2$ are exponential decay rates. Recommended values for $\\beta_1$, $\\beta_2$ and $\\epsilon$ are 0.9, 0.999 and $10^{-8}$ respectively.\n",
        "\n",
        "However, the [authors noted](https://arxiv.org/pdf/1412.6980.pdf) that during the first iterations the method is biased towards zero. Terefore, they used bias-corrected moments defined as:\n",
        "\n",
        "*   First bias-corrected moment (mean): \n",
        "\n",
        "$~~~~~~~~~~~~~~~~~ \\hat{m}^{(k)}=\\frac{m^{(k)}}{1- \\beta_1^{k}}$\n",
        "\n",
        "*   Second moment (uncentered variance):\n",
        "\n",
        "$~~~~~~~~~~~~~~~~~ \\hat{V}^{(k)}= \\frac{V^{(k)}}{1- \\beta_2^{k}}$\n",
        "\n",
        "Note the terms $\\beta_1^{k}$ and $\\beta_2^{k}$ are the beta values to the power of the iteration number. Therefore, the update rule for Adam is:\n",
        "\n",
        "$x^{(k+1)} = x^{(k)} - \\alpha \\frac{\\hat{m}^{(k)}}{\\sqrt{\\hat{V}^{(k)}} + \\epsilon}$\n",
        "\n",
        "<font color='blue'>Implement the **moments equations** and the **update rule** of Adam.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnQ1Err1N5n7"
      },
      "source": [
        "################\n",
        "# --- Adam --- #\n",
        "################\n",
        "\n",
        "def adam(f, x0, grad_f, lr=0.1, beta_1=0.9, beta_2=0.999, eps=1e-8, max_iter=1e5, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    Adam optimization algorithm\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        beta_1   : Exponential decay parameter 1\n",
        "        beta_2   : Exponential decay parameter 2\n",
        "        eps      : Small constant to avoid division over zero\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        plot     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    \n",
        "    # plotting\n",
        "    if traj == True:\n",
        "        x_list = []\n",
        "        f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    m = 0;  V = 0\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter:    \n",
        "        grad_i  = grad_f(f,x)                                           # compute gradient\n",
        "        #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#\n",
        "        m       = 00000                          # Moment 1\n",
        "        V       = 00000                       # Moment 2\n",
        "        m_hat   = 00000                              # Biased-corrected moment 1 \n",
        "        V_hat   = 00000                              # Biased-corrected moment 2\n",
        "        x       = 00000                   # compute step\n",
        "        #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#\n",
        "        iter_i += 1\n",
        "        \n",
        "        # plotting\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "        \n",
        "    print(' Optimization using Adam \\n')\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "    \n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list, \n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ETEwLqQN95v"
      },
      "source": [
        "# --- Adam --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = adam(Rosenbrock_f, x0, central_finite_diff5, lr=0.05, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3za4iNeAOD25"
      },
      "source": [
        "x_array = np.array(x_list).reshape(-1,2)\n",
        "fig  = plt.figure()\n",
        "cs   = plt.scatter(x_array[:,0], x_array[:,1], marker=\".\", c=f_list , cmap=\"seismic\")\n",
        "cbar = fig.colorbar(cs)\n",
        "plt.xlabel(r'$x_1$'); plt.ylabel(r'$x_2$'); plt.title('Rosenbrock with Adam')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}