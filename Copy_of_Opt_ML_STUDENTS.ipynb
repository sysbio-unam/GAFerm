{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Opt_ML_STUDENTS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sysbio-unam/GAFerm/blob/main/Copy_of_Opt_ML_STUDENTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlT5RZ2KxQ5h"
      },
      "source": [
        "# Authors:\n",
        "## Antonio del Rio Chanona      https://www.imperial.ac.uk/people/a.del-rio-chanona\n",
        "## Edgar Ivan Sanchez Medina    https://www.mpi-magdeburg.mpg.de/person/103552/2316\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "from matplotlib.animation import FuncAnimation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTgaIpQpzJXO"
      },
      "source": [
        "Along this notebook you will find <font color='blue'>text in blue that describe the **coding tasks** that you have to implement.</font> Within the code cell, your implementation needs to be between the line:\n",
        "\n",
        "`#-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-#-#`\n",
        "\n",
        "and the line:\n",
        "\n",
        "`#-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#-#-#`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HznFdIzf0rKR"
      },
      "source": [
        "# **1. Optimization basics**\n",
        "\n",
        "An optimization problem has the form\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\min_{x \\in X} \\quad & f(x)\\\\\n",
        "\\textrm{s.t.} \\quad & g_i(x) \\leq 0, i= 1, ..., m\\\\\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "The vector $x = (x_1,..., x_n)$ is the optimization variable of the problem, the function $f : \\mathbb{R}^n \\to \\mathbb{R} $ is the objective function, the functions $g_i : \\mathbb{R}^n \\to \\mathbb{R} $,\n",
        "$i = 1, ...,m$, are the (inequality) constraint functions. A vector $x^â‹†$ is called optimal, if it has the smallest objective value among all vectors\n",
        "that satisfy the constraints.\n",
        "\n",
        "**Convex optimization**\n",
        "\n",
        "A convex optimization problem is one in which the objective and the\n",
        "constraint functions are convex, which means they satisfy the inequality:\n",
        "\n",
        "\\begin{equation}\n",
        "f(\\alpha x + \\beta y) \\leq \\alpha f(x) + \\beta f(y)\n",
        "\\end{equation}\n",
        "\n",
        "for all $x,y \\in \\mathbb{R}^n$ and all $\\alpha, \\beta \\in \\mathbb{R}$ with $\\alpha + \\beta = 1$, $\\alpha  \\geq 0$ and $\\beta  \\geq 0$\n",
        "\n",
        "It is worth nothing that any equality constraint can be represented as two inequality constraints. This enforce that, in a convex problem, the equality constraints must be linear.\n",
        "\n",
        "Therefore, an optimization problem is convex whenever the following requirements are met:\n",
        "\n",
        "* The objective function is convex.\n",
        "* The inequality constraints are convex.\n",
        "* The equality constraints are linear.\n",
        "\n",
        "**Global and local optima**\n",
        "\n",
        "An optimal solution $x^*$ is said to be the global optimum when the constraints are met at this point and $f(x^*) \\leq f(x),  \\forall x \\in X$. When this condition is met only whitin a certain neightborhood, the solution is called local optimum.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzLfKCK36lWz"
      },
      "source": [
        "def Rosenbrock_f(x):\n",
        "    '''\n",
        "    Rosenbrock function\n",
        "    '''\n",
        "    n = np.shape(x)[1]\n",
        "    z = np.sum(100*(x[:,1:] - x[:,:n-1]**2)**2 + (x[:,:n-1] - 1)**2, axis=1)\n",
        "    return z\n",
        "\n",
        "###############################################\n",
        "# --- Central finite differences 5 points --- # \n",
        "###############################################\n",
        "\n",
        "def central_finite_diff5(f, x):\n",
        "      '''\n",
        "      Five-points method for central finite differences.\n",
        "      INPUTS:\n",
        "          f  : Function\n",
        "          x  : Position where to approximate the gradient\n",
        "      OUTPUTS:\n",
        "          grad: Approximation of the gradient of f at x \n",
        "      '''\n",
        "      dim = x.shape[1]\n",
        "      # Step-size is taken as the square root of the machine precision\n",
        "      eps  = np.sqrt(np.finfo(float).eps) \n",
        "      grad = np.zeros((1,dim))\n",
        "        \n",
        "      for i in range(dim):\n",
        "          e           = np.zeros((1,dim))\n",
        "          e[0,i]      = eps\n",
        "          grad_approx = (f(x - 2*e) - 8*f(x - e) + 8*f(x + e) - f(x + 2*e) )/(12*eps) \n",
        "          \n",
        "          grad[0,i]     = grad_approx\n",
        "        \n",
        "      return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYSbDpor27Nf"
      },
      "source": [
        "# **10. Newton's method**\n",
        "\n",
        "This optimization method calculates the inverse of the Hessian matrix to obtain faster convergence than the first-order gradient descent methods.\n",
        "\n",
        "The update rule of the Newton's method is:\n",
        "\n",
        "$ x^{(k+1)} = x^{(k)} - \\left(\\nabla^2 f(x^{(k)}) \\right)^{-1}\\nabla f(x^{(k)}) $\n",
        "\n",
        "The second term from the right hand side of the above equation is then the Newton step, and it requires, from the inverse of the Hessian, for the Hessian matrix to be positive definite. However, for some non-convex problems the Hessian might not be invertible.\n",
        "\n",
        "Similarly as what we did for approximating the gradient using central finite differences, we can approximate the Hessian. For the details and formulas of the Hessian approximation look [here](https://en.wikipedia.org/wiki/Finite_difference)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpLbclJtOUSS"
      },
      "source": [
        "###################################################\n",
        "# --- Central second order finite differences --- #\n",
        "###################################################\n",
        "\n",
        "def Second_diff_fxx(f, x):\n",
        "    '''\n",
        "      Central finite differences approximation of Hessian\n",
        "      INPUTS:\n",
        "          f  : Function\n",
        "          x  : Position where to approximate the Hessian\n",
        "      OUTPUTS:\n",
        "          Hxx: Approximation of the Hessian of f at x \n",
        "      '''\n",
        "\n",
        "    dim   = np.shape(x)[1]\n",
        "    # Step-size is taken as the square root of the machine precision\n",
        "    eps  = np.sqrt(np.finfo(float).eps)\n",
        "    Hxx   = np.zeros((dim,dim))\n",
        "    \n",
        "    for j in range(dim):\n",
        "        # compute Fxx (diagonal elements)\n",
        "        x_d_f       = np.copy(x)             # forward step\n",
        "        x_d_b       = np.copy(x)             # backward step\n",
        "        x_d_f[0,j]  = x_d_f[0,j] + eps\n",
        "        x_d_b[0,j]  = x_d_b[0,j] - eps\n",
        "        Hxx[j,j]    = (f(x_d_f) -2*f(x) + f(x_d_b))/eps**2\n",
        "\n",
        "        for i in range(j+1,dim):\n",
        "            # compute Fxy (off-diagonal elements)\n",
        "            # Fxy\n",
        "            x_d_fxfy    = np.copy(x_d_f)\n",
        "            x_d_fxfy[0,i] = x_d_fxfy[0,i] + eps\n",
        "            x_d_fxby    = np.copy(x_d_f)\n",
        "            x_d_fxby[0,i] = x_d_fxby[0,i] - eps\n",
        "            x_d_bxfy    = np.copy(x_d_b)\n",
        "            x_d_bxfy[0,i] = x_d_bxfy[0,i] + eps\n",
        "            x_d_bxby    = np.copy(x_d_b)\n",
        "            x_d_bxby[0,i] = x_d_bxby[0,i] - eps\n",
        "            Hxx[j,i]    = (f(x_d_fxfy) - f(x_d_fxby) - f(x_d_bxfy) + f(x_d_bxby))/(4*eps**2)\n",
        "            Hxx[i,j]    = Hxx[j,i]\n",
        "\n",
        "    return Hxx\n",
        "\n",
        "\n",
        "x1       = np.array([2.,2.]).reshape(1,-1)\n",
        "Hxx = Second_diff_fxx(Rosenbrock_f, x1)\n",
        "print('Hxx = ',Hxx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1Dmi0IgO0-c"
      },
      "source": [
        "###########################\n",
        "# --- Newton's Method --- #\n",
        "###########################\n",
        "\n",
        "def newton(f, x0, grad_f, H_f, max_iter=1e3, grad_tol=1e-4, traj=False):\n",
        "    '''\n",
        "    Newton's method\n",
        "\n",
        "    Note: the Hessian can become ill-conditioned\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        H_f      : Hessian function\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        traj     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    n      = np.shape(x0)[0]\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    grad_i = grad_tol*10\n",
        "    \n",
        "    # trajectory\n",
        "    if traj == True:\n",
        "            x_list = []\n",
        "            f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    while np.sum(np.abs(grad_i)) > grad_tol and iter_i < max_iter: \n",
        "              \n",
        "        #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-#-#      \n",
        "        grad_i  = 00000                         # compute gradient\n",
        "        Hxx     = 00000                            # compute Hessian\n",
        "        x       = 00000 # update  \n",
        "        #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#-#            \n",
        "        iter_i += 1\n",
        "        \n",
        "        # trajectory\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "    \n",
        "    print(\" Optimization using Newton's method \\n\")\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "    print('Final grad: ', grad_i)\n",
        "\n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list\n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3V7J59oO5Sa"
      },
      "source": [
        "# --- Newton's method --- #\n",
        "x0 = np.array([0.,0.]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = newton(Rosenbrock_f, x0, central_finite_diff5, Second_diff_fxx, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btYaAciHO9Ck"
      },
      "source": [
        "x_array = np.array(x_list).reshape(-1,2)\n",
        "fig  = plt.figure()\n",
        "cs   = plt.scatter(x_array[:,0], x_array[:,1], marker=\".\", c=f_list , cmap=\"seismic\")\n",
        "cbar = fig.colorbar(cs)\n",
        "plt.xlabel(r'$x_1$'); plt.ylabel(r'$x_2$'); plt.title(\"Rosenbrock with Newton's method\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb3QmNBnPAb_"
      },
      "source": [
        "Of course, using the Hessian information accelerates the optimization significantly! But, this comes at the expense of not only obtaining the Hessian, but also inverting it! This is the reason why second-order methods are not used in practice with large datasets. But, if the dataset is relatively small, go ahead and use second-order methods (whenever you have access to the second derivatives, of course)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NITxr1Ex3B67"
      },
      "source": [
        "# **11. Constrained Newton's method**\n",
        "\n",
        "If we recall the Newton's method, the step (called Newton step) that we take at each iteration is:\n",
        "\n",
        "\\begin{equation}\n",
        "\\Delta x = - \\left(\\nabla^2 f(x^{(k)}) \\right)^{-1}\\nabla f(x^{(k)})\n",
        "\\end{equation}\n",
        "\n",
        "A nice interpretation of the Newtonâ€™s step is to set\n",
        "it by minimizing the second-order approximation of $f$ at $x$.\n",
        "In case of (equality) constrained problems, the Newton's method is slightly modified in two main aspects:\n",
        "\n",
        "* The initial point must be feasible.\n",
        "* The definition of Newton step is modified to make sure that the newton step is a feasible direction of the problem.\n",
        "\n",
        "To derive the Constrained Newton's method the objective function in the original problem:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\min_{x \\in X} \\quad & f(x)\\\\\n",
        "\\textrm{s.t.} \\quad & h_i(x) = 0, i= 1, ..., m\\\\\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "is approximated by the second-order Taylor expansion:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\min_{\\Delta x \\in X} \\quad & \\hat{f}(x + \\Delta x) = f(x) + \\nabla f(x)^T \\Delta x + \\frac{1}{2} \\Delta x ^T \\nabla^2 f(x) \\Delta x\\\\\n",
        "\\textrm{s.t.} \\quad & h_i(x + \\Delta x) = 0, i= 1, ..., m\\\\\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "The Newton step $\\Delta x$ now can be obtained by solving the following system of equations: \n",
        "\n",
        "\n",
        "\\begin{gather}\n",
        " \\begin{bmatrix} \n",
        " \\nabla^2 f(x) & \\nabla h(x)^T \\\\ \n",
        " \\nabla h(x) & 0 \n",
        " \\end{bmatrix}\n",
        " \\begin{bmatrix} \n",
        " \\Delta x \\\\ \n",
        " \\lambda \n",
        " \\end{bmatrix}\n",
        " =\n",
        " -\n",
        " \\begin{bmatrix} \n",
        " \\nabla f(x) \\\\ \n",
        " h(x) \n",
        " \\end{bmatrix}\n",
        "\\end{gather}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpG_QQ_F0vlD"
      },
      "source": [
        "##############################\n",
        "# --- Objective function --- #\n",
        "##############################\n",
        "\n",
        "def Obj(x):\n",
        "  '''\n",
        "  Objective function\n",
        "  '''\n",
        "  return x[:,0]**2 + 5*x[:,1]**2\n",
        "\n",
        "##################################\n",
        "# --- Gradient Obj. function --- #\n",
        "##################################\n",
        "\n",
        "def gradient(x):\n",
        "  '''\n",
        "  Gradient of objective function\n",
        "  '''\n",
        "  g1 = 2*x[:,0]\n",
        "  g2 = 10*x[:,1]\n",
        "  return  np.array([g1,g2])\n",
        "\n",
        "###################\n",
        "# --- Hessian --- #\n",
        "###################\n",
        "exact_hess = np.array([[2,0],[0, 10]]) \n",
        "\n",
        "################################\n",
        "# --- Equality constrained --- #\n",
        "################################\n",
        "\n",
        "def Const(x):\n",
        "  '''\n",
        "  Equality constrained\n",
        "  '''\n",
        "  return -x[:,0] - x[:,1] - 2\n",
        "\n",
        "#########################\n",
        "# --- Plot function --- #\n",
        "#########################\n",
        "\n",
        "x_1 = np.linspace(-4,4)\n",
        "x_2 = np.linspace(-2,2)\n",
        "x = np.vstack((x_1,x_2)).reshape(-1,2)\n",
        "X, Y = np.meshgrid(x_1, x_2)\n",
        "Z = Obj(np.append(X.reshape(-1,1), Y.reshape(-1,1), axis=1))\n",
        "Z = Z.reshape(X.shape)\n",
        "\n",
        "const_val = Const(x)\n",
        "\n",
        "fig = plt.figure(figsize=(6,5))\n",
        "left, bottom, width, height = 0.15, 0.1, 0.8, 0.8\n",
        "ax = fig.add_axes([left, bottom, width, height]) \n",
        "contours = ax.contour(x_1, x_2, Z, colors='black', alpha=0.8)\n",
        "ax.clabel(contours, inline=True, fontsize=8)\n",
        "ax.set_xlabel(r'$x_1$')\n",
        "ax.set_ylabel(r'$x_2$')\n",
        "plt.xlim(-4, 4)\n",
        "plt.ylim(-2, 2)\n",
        "plt.plot(x_1, -x_1-2, 'b-',label='h(x)=0')\n",
        "plt.legend()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfcQSjdL3_Hg"
      },
      "source": [
        "# Using Scipy SLSQP to see what to expect\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "def obj_f(x):\n",
        "  return x[0]**2 + 5*x[1]**2\n",
        "\n",
        "initial = np.array([-2, 0])\n",
        "cons = ({'type': 'eq', 'fun': lambda x:  -x[0] -x[1] - 2})\n",
        "\n",
        "res = minimize(obj_f, initial, method='SLSQP', tol=1e-15, constraints=cons)\n",
        "print(res.fun)\n",
        "print(res.x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQKdIvp55MjQ"
      },
      "source": [
        "###################################\n",
        "# --- Constrained Newton step --- #\n",
        "###################################\n",
        "\n",
        "def Const_Newton_step(H, J, g, h):\n",
        "  '''\n",
        "  Determine the constrained Newton step\n",
        "\n",
        "    INPUTS:\n",
        "      H: Hessian matrix of the objective function at x\n",
        "      J: Jacobian of constraints at x\n",
        "      g: Gradient of objective function at x\n",
        "      h: Constraints value at x\n",
        "        \n",
        "    OUTPUTS:\n",
        "      step: Constrained Newton step\n",
        "      lambda: Lagrange multipliers\n",
        "  '''\n",
        "  dim = H.shape[0]\n",
        "  num_const = J.shape[0]\n",
        "  dim_lag = dim + num_const\n",
        "\n",
        "  A = np.zeros((dim_lag, dim_lag))\n",
        "  A[:dim,:dim] = H; \n",
        "  A[dim:,:dim][0] = J; \n",
        "  A[:dim,dim:][:,0] = J.T.reshape(dim,)\n",
        "  B     = -np.append(g, h)\n",
        "  sol   = np.linalg.solve(A, B)\n",
        "  step  = sol[:dim]\n",
        "  lamda = sol[dim:]\n",
        "\n",
        "  return step, lamda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_5gDdaV0UEI"
      },
      "source": [
        "#######################################\n",
        "# --- Constrained Newton's Method --- #\n",
        "#######################################\n",
        "\n",
        "def const_newton(f, const, x0, grad_f, H_f, max_iter=1e3, tol=1e-4, traj=False):\n",
        "    '''\n",
        "    Constrained Newton's method\n",
        "\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        const    : Constraint\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        H_f      : Hessian function\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        traj     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    decr   = 10\n",
        "    \n",
        "    # trajectory\n",
        "    if traj == True:\n",
        "            x_list = []\n",
        "            f_list = []            \n",
        "    \n",
        "    # optimization loop\n",
        "    while decr/2 > tol and iter_i < max_iter:\n",
        "        #H       = H_f(f, x)                             # compute Hessian of Obj function\n",
        "\n",
        "        #-#-#-#-#-#-#-#-#-# Start of your code #-#-#-#-#-#-#-#-#-# \n",
        "        H       = 00000\n",
        "        J       = 00000     # compute Jacobian of constraint\n",
        "        g       = 00000                             # compute gradient of Obj function\n",
        "        h       = 00000                              # compute constraint value                     \n",
        "        delta_x, lamb = 00000      # compute step and lambda\n",
        "        x       = 00000                           # update \n",
        "        decr    = 00000               # decrement  \n",
        "        #-#-#-#-#-#-#-#-#-# End of your code #-#-#-#-#-#-#-#-#-#   \n",
        "                \n",
        "        iter_i += 1\n",
        "        \n",
        "        # trajectory\n",
        "        if traj == True:\n",
        "            x_list.append(x.flatten().tolist())\n",
        "            f_list.append(f(x))\n",
        "    \n",
        "    print(\" Optimization using constrained Newton's method \\n\")\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "\n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list\n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGXUPLX0wBU3"
      },
      "source": [
        "# --- Constrained Newton's method --- #\n",
        "x0 = np.array([3,1.5]).reshape(1,-1)\n",
        "\n",
        "xf, x_list, f_list = const_newton(Obj, Const, x0, gradient, Second_diff_fxx, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUgJhLKUIew4"
      },
      "source": [
        "fig = plt.figure(figsize=(6,5))\n",
        "left, bottom, width, height = 0.15, 0.1, 0.8, 0.8\n",
        "ax = fig.add_axes([left, bottom, width, height]) \n",
        "contours = ax.contour(x_1, x_2, Z, colors='black', alpha=0.8)\n",
        "ax.clabel(contours, inline=True, fontsize=8)\n",
        "ax.set_xlabel(r'$x_1$')\n",
        "ax.set_ylabel(r'$x_2$')\n",
        "plt.xlim(-4, 4)\n",
        "plt.ylim(-2, 2)\n",
        "plt.plot(x_1, -x_1-2, 'b-',label='h(x)=0')\n",
        "plt.plot(x0[:,0], x0[:,1], 'ro', label='initial point')\n",
        "plt.plot(x_list[1][0], x_list[1][1], 'r*', label='Optimum', ms=10)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMub0N973Nh6"
      },
      "source": [
        "# **12. Barrier method (interior-point methods)**\n",
        "\n",
        "With the Constrained Newton's method we accomplished to solve optimization problem with equality constraints. With interior-point methods we can solve convex problems that include inequality constraints. Interior-point methods solve this type of problems by solving a sequence of equality constrained problems using Newton's method. The barrier method is one of these interior-point algorithms, and is the one we are going to focus here.\n",
        "\n",
        "The goal is to approximately formulate the inequality constrained problem \n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\min_{x \\in X} \\quad & f(x)\\\\\n",
        "\\textrm{s.t.} \\quad & h_i(x) = 0, i= 1, ..., m\\\\\n",
        "\\quad & g_j(x) \\leq 0, j=1,...,n\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "as an equality constrained problem to which Newtonâ€™s method can be applied. This is accomplished by making the inequality constraints $g_j(x)$ implicit in the objective function $f(x)$.\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\min_{x \\in X} \\quad & f(x) + \\sum_{j=1}^{n}I_-(g_j(x))\\\\\n",
        "\\textrm{s.t.} \\quad & h_i(x) = 0, i= 1, ..., m\\\\\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "where $I_-$ is the indicator function for the nonpositive reals:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "    I_-(u)= \n",
        "\\begin{cases}\n",
        "    0, &  u \\leq 0\\\\\n",
        "    \\infty,    & u > 0\n",
        "\\end{cases}\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "The problem with this formulation is that, even when we accomplished to make the inequality constraints implicit in the objective function, this objective function is not (in general) differentiable, so Newton's method cannot be applied. The basic idea of the barrier method is to approximate this indicator function as:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\hat{I}_-(u) = - \\frac{1}{t} \\log (-u) \n",
        "\\end{equation}\n",
        "\n",
        "Therefore, the approximated problem that is going to be solved is:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\min_{x \\in X} \\quad & f(x) + \\sum_{j=1}^{n}- \\frac{1}{t} \\log (-g_j(x))\\\\\n",
        "\\textrm{s.t.} \\quad & h_i(x) = 0, i= 1, ..., m\\\\\n",
        "\\end{aligned}\n",
        "\\end{equation}\n",
        "\n",
        "where the function $\\phi(x) = - \\sum_{j=1}^{n} \\log (-g_j(x))$ is called the *logarithmic barrier* for the problem.\n",
        "\n",
        "The accuracy of this approximation improves as the parameter $t$ increases. However, when the parameter $t$ is large, the objective function\n",
        "is difficult to minimize by Newtonâ€™s method, since its Hessian varies rapidly near the boundary of the feasible set. This problem can be overcomed by solving a sequence of problems of the same form, increasing the parameter $t$ (and therefore the accuracy of the approximation) at each step, and starting each Newton minimization at the solution of the problem for the previous value of t.\n",
        "\n",
        "The gradient and the Hessian of the logarithmic barrier function are given by:\n",
        "\n",
        "\\begin{equation}\n",
        "    \\nabla \\phi (x) = \\sum_{j=1}^{n} - \\frac{1}{g_j(x)} \\nabla g_j(x)\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "    \\nabla^2 \\phi (x) = \\sum_{j=1}^{n} \\frac{1}{g_j(x)^2} \\nabla g_j(x) \\nabla g_j(x)^T - \\sum_{j=1}^{n} \\frac{1}{g_j(x)} \\nabla^2 g_j(x)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "**Algorithm**\n",
        "\n",
        "1. Given a strictly feasible starting point $x \\in X$, $t := t^{(0)}$ $\\alpha > 0$, $\\epsilon >0$.\n",
        "2. Repeat until $n/t \\leq \\epsilon$\n",
        "3. $~~~~~~$ Solve approximated problem with $t:= t^{(k)}$ starting from $x^{k}$\n",
        "2. $~~~~~~$ Update $x^{k+1} := x^{*(k)}$\n",
        "3. $~~~~~~$ Increase $t^{k+1} := \\alpha t^{(k)}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rivDA_3ThIzj"
      },
      "source": [
        "The example problem to solve is:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{aligned}\n",
        "\\min_{x_1, x_2 \\in X} \\quad & x_1^2 + 5x_2^2\\\\\n",
        "\\textrm{s.t.} \\quad & -x_1-x_2-2 = 0\\\\\n",
        "& -x_1-x_2 \\leq 0\n",
        "\\end{aligned}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQur-ADLk_x2"
      },
      "source": [
        "# Inequality constraint\n",
        "def Ineq_const(x):\n",
        "  '''\n",
        "  Inequality constraint\n",
        "  '''\n",
        "  return -x[:,0] + x[:,1]\n",
        "\n",
        "grad_ineq = np.array([-1, 1])           # Gradient of inequality constraint\n",
        "hess_ineq = np.array([[0,0], [0,0]])    # Hessian of inequality constraint\n",
        "\n",
        "# Plot\n",
        "x = np.vstack((x_1,x_2)).reshape(-1,2)\n",
        "ineq_const_val = Ineq_const(x)\n",
        "fig = plt.figure(figsize=(6,5))\n",
        "left, bottom, width, height = 0.15, 0.1, 0.8, 0.8\n",
        "ax = fig.add_axes([left, bottom, width, height]) \n",
        "contours = ax.contour(x_1, x_2, Z, colors='black', alpha=0.8)\n",
        "ax.clabel(contours, inline=True, fontsize=8)\n",
        "ax.set_xlabel(r'$x_1$')\n",
        "ax.set_ylabel(r'$x_2$')\n",
        "plt.xlim(-4, 4)\n",
        "plt.ylim(-2, 2)\n",
        "plt.plot(x_1, -x_1-2, 'b-',label='h(x)=0')\n",
        "plt.plot(x_1, x_1, 'b--',label='g(x)=0')\n",
        "plt.fill_between(x_1, x_1, np.repeat(4, len(x_1)), color='k', alpha=0.3)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQP2haywT8Si"
      },
      "source": [
        "##########################################\n",
        "# --- Approximation with log barrier --- #\n",
        "##########################################\n",
        "\n",
        "def Obj_barrier(x):\n",
        "  '''\n",
        "  Objective function with log barrier\n",
        "  '''\n",
        "  t = 0.1\n",
        "  phi = - np.log(-Ineq_const(x))\n",
        "  return x[:,0]**2 + (5*x[:,1])**2 + 1/t*phi\n",
        "\n",
        "\n",
        "Z2 = Obj_barrier(np.append(X.reshape(-1,1), Y.reshape(-1,1), axis=1))\n",
        "Z2 = Z2.reshape(X.shape)\n",
        "\n",
        "const_val = Const(x)\n",
        "x = np.vstack((x_1,x_2)).reshape(-1,2)\n",
        "ineq_const_val = Ineq_const(x)\n",
        "\n",
        "# Hacer subplots\n",
        "fig = plt.figure(figsize=(6,5))\n",
        "left, bottom, width, height = 0.15, 0.1, 0.8, 0.8\n",
        "ax = fig.add_axes([left, bottom, width, height]) \n",
        "contours = ax.contour(x_1, x_2, Z, colors='black', alpha=0.8)\n",
        "plt.contour(x_1, x_2, Z2, colors='green', alpha=0.8)\n",
        "ax.clabel(contours, inline=True, fontsize=8)\n",
        "ax.set_xlabel(r'$x_1$')\n",
        "ax.set_ylabel(r'$x_2$')\n",
        "plt.xlim(-4, 4)\n",
        "plt.ylim(-2, 2)\n",
        "plt.plot(x_1, -x_1-2, 'b-',label='h(x)=0')\n",
        "plt.plot(x_1, x_1, 'b--',label='g(x)=0')\n",
        "plt.fill_between(x_1, x_1, np.repeat(4, len(x_1)), color='k', alpha=0.3)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7ghxMlVsjGU"
      },
      "source": [
        "# Using Scipy SLSQP to see what to expect\n",
        "\n",
        "from scipy.optimize import minimize\n",
        "\n",
        "def obj_f(x):\n",
        "  return x[0]**2 + 5*x[1]**2\n",
        "\n",
        "initial = np.array([-2, 0])\n",
        "cons = ({'type': 'eq', 'fun': lambda x:  -x[0] -x[1] - 2},\n",
        "        {'type': 'ineq', 'fun': lambda x:  x[0] -x[1]})\n",
        "\n",
        "res = minimize(obj_f, initial, method='SLSQP', tol=1e-15, constraints=cons)\n",
        "print(res.fun)\n",
        "print(res.x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6Ma-ghfjBhG"
      },
      "source": [
        "#######################################\n",
        "# --- Hessian logarithmic barrier --- #\n",
        "#######################################\n",
        "\n",
        "def hess_logbarrier(x):\n",
        "  '''\n",
        "  Hessian of the logarithmic barrier function\n",
        "  '''\n",
        "  val_ineq = Ineq_const(x)[0]\n",
        "  h1 = 1/(val_ineq**2) * grad_ineq.reshape(-1,1) @ grad_ineq.reshape(-1,1).T \n",
        "  h2 = - 1/(val_ineq)*hess_ineq\n",
        "  return h1 + h2\n",
        "\n",
        "########################################\n",
        "# --- Gradient logarithmic barrier --- #\n",
        "########################################\n",
        "\n",
        "def grad_logbarrier(x):\n",
        "  '''\n",
        "  Gradient of the logarithmic barrier function\n",
        "  '''\n",
        "  val_ineq = Ineq_const(x)[0]\n",
        "  return -1/val_ineq*grad_ineq\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDFNdhJV2iDb"
      },
      "source": [
        "###################################\n",
        "# --- Constrained Newton step --- #\n",
        "###################################\n",
        "\n",
        "def DeltaX(H, J, g, h):\n",
        "  '''\n",
        "  Determine the constrained Newton step\n",
        "\n",
        "    INPUTS:\n",
        "      H: Hessian matrix of the objective function at x\n",
        "      J: Jacobian of constraints at x\n",
        "      g: Gradient of objective function at x\n",
        "      h: Constraints value at x\n",
        "        \n",
        "    OUTPUTS:\n",
        "      step: Constrained Newton step\n",
        "      lambda: Lagrange multipliers\n",
        "  '''\n",
        "  dim = H.shape[0]\n",
        "  num_const = J.shape[0]\n",
        "  dim_lag = dim + num_const\n",
        "\n",
        "  A = np.zeros((dim_lag, dim_lag))\n",
        "  A[:dim,:dim] = H; \n",
        "  A[dim:,:dim][0] = J; \n",
        "  A[:dim,dim:][:,0] = J.T.reshape(dim,)\n",
        "  B     = -np.append(g, h)\n",
        "  sol   = np.linalg.solve(A,B)\n",
        "  step  = sol[:dim]\n",
        "  lamda = sol[dim:]\n",
        "\n",
        "  return step, lamda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAkKWp7EHwND"
      },
      "source": [
        "##########################\n",
        "# --- Barrier method --- #\n",
        "##########################\n",
        "\n",
        "def barrier(f, eq_const, x0, grad_f, t, alpha=1.01, max_iter=1e4, tol=1e-4, traj=False):\n",
        "    '''\n",
        "    Barrier method (interior-point methods)\n",
        "\n",
        "    INPUTS:\n",
        "        f        : Function\n",
        "        eq_const : Equality constraint\n",
        "        x0       : Initial guess\n",
        "        grad_f   : Gradient function\n",
        "        t        : Barrier parameter\n",
        "        max_iter : Maximum number of iterations\n",
        "        grad_tol : Tolerance for gradient approximation\n",
        "        traj     : Boolean for plotting\n",
        "    OUTPUTS:\n",
        "        x        : Optimal point\n",
        "        iter_i   : Number of iterations needed\n",
        "    '''\n",
        "    \n",
        "    # initialize problem\n",
        "    x      = np.copy(x0)\n",
        "    iter_i = 0\n",
        "    n_iq   = 1             # number of inequality constraints\n",
        "    \n",
        "    # trajectory\n",
        "    if traj == True:\n",
        "            x_list = []\n",
        "            f_list = []            \n",
        "\n",
        "    # optimization loop\n",
        "    while n_iq/t >= tol*0.001 and iter_i < max_iter: \n",
        "      decr = 10\n",
        "      while decr/2 > tol and iter_i < max_iter:\n",
        "        J   = np.array([-1, -1]).reshape(1,-1)     # compute Jacobian of constraint\n",
        "        h   = eq_const(x)                          # compute constraint value\n",
        "        \n",
        "        x_1 = x[:,0][0]\n",
        "        x_2 = x[:,1][0]\n",
        "        g = np.array([2*x_1 - 1/t*x_1/(x_1-x_2), 10*x_2 + 1/t*x_2/(x_1-x_2)]).reshape(-1,1) \n",
        "        \n",
        "        h1 = 2 + 1/t*x_2/(x_1 - x_2)**2\n",
        "        h2 = -1/t*x_1/(x_1 - x_2)**2\n",
        "        h3 = -1/t*x_2/(x_1 - x_2)**2\n",
        "        h4 = 10 + 1/t*x_1/(x_1 - x_2)**2\n",
        "        H_f = np.array([[h1, h2], [h3, h4]])                \n",
        "        \n",
        "        #delta_x = -np.matmul(np.linalg.pinv(H_f),g) \n",
        "        #delta_x, la = Const_Newton_step(H_f,J,g,h)      # compute step and lambda\n",
        "        delta_x, la = DeltaX(H_f,J,g,h)\n",
        "        x       = x + delta_x                             # update x\n",
        "        decr    = delta_x.T @ H_f @ delta_x               # decrement\n",
        "        iter_i += 1\n",
        "      t       = t * alpha                             # update t\n",
        "      # trajectory\n",
        "      if traj == True:\n",
        "          x_list.append(x.flatten().tolist())\n",
        "          f_list.append(f(x))\n",
        "    \n",
        "    print(\" Optimization using barrier method \\n\")\n",
        "    print('Iterations: ', iter_i)\n",
        "    print('Optimal x : ', x) \n",
        "\n",
        "    # trajectory    \n",
        "    if traj == True:\n",
        "        return x, x_list, f_list\n",
        "        \n",
        "    return x, iter_i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYra2hMcBxNP"
      },
      "source": [
        "# --- Barrier method --- #\n",
        "x0 = np.array([3.5,1.5]).reshape(1,-1)\n",
        "t0 = 0.0001\n",
        "\n",
        "xf, x_list, f_list = barrier(Obj, Const, x0, gradient, t0, traj=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-Wwch13EKQJ"
      },
      "source": [
        "fig = plt.figure(figsize=(6,5))\n",
        "left, bottom, width, height = 0.15, 0.1, 0.8, 0.8\n",
        "ax = fig.add_axes([left, bottom, width, height]) \n",
        "contours = ax.contour(x_1, x_2, Z, colors='black', alpha=0.8)\n",
        "ax.clabel(contours, inline=True, fontsize=8)\n",
        "ax.set_xlabel(r'$x_1$')\n",
        "ax.set_ylabel(r'$x_2$')\n",
        "plt.xlim(-4, 4)\n",
        "plt.ylim(-2, 2)\n",
        "plt.plot(x0[:,0], x0[:,1], 'ro', label='initial point')\n",
        "plt.plot(x_list[-1][0], x_list[-1][1], 'r*', label='Optimum', ms=10)\n",
        "plt.plot(x_1, -x_1-2, 'b-',label='h(x)=0')\n",
        "plt.plot(x_1, x_1, 'b--',label='g(x)=0')\n",
        "plt.fill_between(x_1, x_1, np.repeat(4, len(x_1)), color='k', alpha=0.3)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}